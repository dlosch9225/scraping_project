# ğŸŒ Multi-Source Data Scraper

This Python project automatically scrapes and stores daily data from multiple public sources:
- ğŸ’° **Bitcoin price** from CoinGecko  
- ğŸŒ¤ï¸ **Weather data** from Open-Meteo  
- ğŸŒ **Earthquake alerts** from USGS

Data is saved in both **CSV** and **HDF5** formats. A scheduler runs the scraper daily during a 9-day window. Logs and visualizations are included.

---

## ğŸ“š Table of Contents

- [âœ¨ Features](#-features)
- [ğŸ—ï¸ Project Structure](#ï¸-project-structure)
- [âš™ï¸ Installation](#ï¸-installation)
- [ğŸš€ Usage](#-usage)
- [â° Automation](#-automation)
- [ğŸ’¾ Data Storage Format](#-data-storage-format)
- [ğŸŒ Data Source Metadata](#-data-source-metadata)
- [âš–ï¸ Legal & Ethical Compliance](#ï¸-legal--ethical-compliance)
- [ğŸ—‚ Data Structure Example](#-data-structure-example)
- [ğŸ“Š Visualization](#-visualization)
- [ğŸ§© Value Added Through Visualization](#-value-added-through-visualization)
- [ğŸ“ˆ Sample Graphs](#-sample-graphs)
- [ğŸ§ª Testing & Validation](#-testing--validation)
- [â— Known Limitations](#-known-limitations)
- [ğŸ“¦ Requirements](#-requirements)
- [ğŸ”­ Meta Perspective & Comparison with Best Practices](#-meta-perspective--comparison-with-best-practices)
- [ğŸ“„ License](#-license)
- [ğŸ“¬ Contact](#-contact)

---

## âœ¨ Features

- â±ï¸ **Automated daily scraping** at 11:00 AM (via `cron`).
- ğŸ’¾ **Dual storage**: CSV + HDF5 with deduplication.
- ğŸ“Š **Clean and styled visualizations** with Matplotlib.
- ğŸ§ª Modular, testable, and easy to expand.

---

## ğŸ—ï¸ Project Structure
Below is the real folder structure of this project. This structure supports modular development, maintenance, and reproducibility.

```
scraping_project/
â”œâ”€â”€ data/                # All collected data (CSV + main HDF5 storage)
â”œâ”€â”€ logs/                # Logs from scrapers and scheduler
â”œâ”€â”€ scrapers/            # Web scrapers for Bitcoin, weather, earthquakes
â”œâ”€â”€ plotting/            # Scripts to visualize time-series data
â”œâ”€â”€ scripts/             # Utility tools: backup, inspect, update HDF5, etc.
â”œâ”€â”€ backups/             # Timestamped HDF5 dataset backups
â”œâ”€â”€ storage.py           # Central HDF5 handling (read/write, deduplication)
â”œâ”€â”€ scheduler.py         # Daily task manager (used with cron)
â”œâ”€â”€ start_scheduler.sh   # Launch script for automation via cron
â”œâ”€â”€ websites.csv         # Metadata of all scraped sources
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md            # Project documentation 

```

## âš™ï¸ Installation

#### 1.  The repo:
```
git clone https://github.com/your_username/scraping_project.git
cd scraping_project
```

#### 2. Create a virtual environment:
```
python3 -m venv .venv
source .venv/bin/activate
```

#### 3. Install dependencies:
```
pip install -r requirements.txt
```
## ğŸš€ Usage

Run the scraper manually:
```
python scrapers/scraper.py
```

Or run the visualization:
```
python plotting/plot_bitcoin.py
```
## â° Automation

The project uses cron to:

* Start the scheduler at system reboot.
* Trigger scrapers daily at 11:00 AM.

#### Cron Configuration Example (macOS/Linux)
```
@reboot /absolute/path/to/start_scheduler.sh
0 11 * * * /absolute/path/to/start_scheduler.sh
```

Ensure:

* start_scheduler.sh is executable (chmod +x).
* The Python virtual environment path is correct.
* System is awake at 11:00 AM.

## ğŸ’¾ Data Storage Format

The project uses both CSV and HDF5 for persistent storage.

### Why HDF5?

* ğŸ” Fast reading/writing of large tables.
* ğŸ“… Easy time-based filtering.
* ğŸ”’ Deduplication by date.
* ğŸ”— Integrated with pandas.HDFStore.

#### HDF5 File Structure:

| Data Source      | HDF5 Key      | CSV File              |
| ---------------- | ------------- | --------------------- |
| CoinGecko BTC    | `bitcoin`     | `data/bitcoin.csv`    |
| Open-Meteo       | `weather`     | `data/open_meteo.csv` |
| USGS Earthquakes | `earthquakes` | `data/usgs.csv`       |


## ğŸŒ Data Source Metadata

The metadata for all web sources is stored in [`websites.csv`](./websites.csv), which includes:

- âœ… Website name
- âœ… URL
- âœ… Description
- âœ… Type of access (API or webpage)
- âœ… Notes on compliance and usage terms

This allows for easy replacement or expansion of data sources without changing the Python code.

## âš–ï¸ Legal & Ethical Compliance

All data sources used in this project were chosen based on their availability for **public and non-commercial academic use**. Below is a summary:

| Source       | Compliance Notes                                                                                   |
|--------------|----------------------------------------------------------------------------------------------------|
| CoinGecko    | Public API. Free for educational and non-commercial use. No authentication required. [Docs](https://www.coingecko.com/en/api/documentation) |
| Open-Meteo   | Open API with no key required. Designed for open access and academic use. [Docs](https://open-meteo.com/en/docs) |
| USGS         | U.S. Government data. Fully public and freely accessible. [Docs](https://earthquake.usgs.gov/data/comcat/documentation.php) |

The project ensures respectful access patterns:
- Limited to once-daily requests
- Uses official APIs when available
- Respects server limits and usage conditions

âš ï¸ No scraping of private, login-protected, or copyrighted content.


## ğŸ—‚ Data Structure Example

Example row from open_meteo.csv:

| date       | temperature | wind_speed | weather_code | source         |
| ---------- | ----------- | ---------- | ------------ | -------------- |
| 2025-10-27 | 7.8         | 17.9       | 61           | Open-Meteo API |

## ğŸ“Š Visualization

Once data is collected over multiple days, the following graphs are generated:

| Plot                    | Script                     |
| ----------------------- | -------------------------- |
| ğŸ“ˆ Bitcoin Price Trend  | `plotting/plot_bitcoin.py` |
| ğŸŒ¡ï¸ Temp/Wind in Berlin | `plotting/open_meteo.py`   |
| ğŸŒ Daily Earthquakes    | `plotting/plot_usgs.py`    |

Each chart includes:

* ğŸ“… Date-based X-axis.
* ğŸ·ï¸ Proper labeling and units (e.g., $, Â°C, magnitude).
* ğŸ¯ Clean aesthetics and layout.
* ğŸ” Fallback to .csv if HDF5 fails.

## ğŸ§© Value Added Through Visualization

Visualizing numeric trends over time provides actionable insights that static values do not.

Examples:
- ğŸ’° **Bitcoin volatility** shows the potential impact on financial planning or investment strategies.
- ğŸŒ¤ï¸ **Weather changes** could influence logistics, events, or energy management.
- ğŸŒ **Earthquake frequency** offers early indications of potential seismic clusters.

The automated visualizations produced by this system allow decision-makers to monitor key variables over time, helping move from reactive to proactive decisions.

## ğŸ“ˆ Sample Graphs

### ğŸ’° Bitcoin Price

![Bitcoin](plotting/plots/bitcoin_plot.png)

### ğŸŒ¤ï¸ Berlin Weather (Temperature and Wind)

![Weather](plotting/plots/open_meteo_plot.png)

### ğŸŒ Earthquake Magnitudes

![Earthquakes](plotting/plots/usgs_plot.png)

## ğŸ§ª Testing & Validation

* Manual insertion of synthetic data for plotting.
* Tested fallback to CSV when HDF5 unavailable.
* Verified scheduler triggers scraping.
* Handled API downtime with retry logic.
* Verified deduplication in CSV and HDF5.

## â— Known Limitations

* ğŸ’¡ System must be on (not sleeping) at 11:00 AM for cron to run.
* ğŸ”Œ Internet connection required for scraping APIs.
* ğŸ§  Requires periodic check of scheduler_state.txt.
* ğŸ› ï¸ Scheduler state does not reset automatically after 9 days.

## ğŸ“¦ Requirements

Install via:
```
pip install -r requirements.txt
```
#### Core Dependencies
```
beautifulsoup4==4.14.2
certifi==2025.10.5
charset-normalizer==3.4.4
contourpy==1.3.3
cycler==0.12.1
fonttools==4.60.1
h5py==3.15.1
idna==3.11
kiwisolver==1.4.9
matplotlib==3.10.7
numpy==2.3.4
packaging==25.0
pandas==2.3.3
pandas-stubs==2.3.2.250926
pillow==12.0.0
pyparsing==3.2.5
python-dateutil==2.9.0.post0
pytz==2025.2
requests==2.32.5
schedule==1.2.2
seaborn==0.13.2
six==1.17.0
soupsieve==2.8
typing_extensions==4.15.0
tzdata==2025.2
urllib3==2.5.0
```

## ğŸ”­ Meta Perspective & Comparison with Best Practices

This project follows industry-standard practices for data scraping, automation, and storage:

| Practice Area         | Applied Approach in Project                                   |
|----------------------|---------------------------------------------------------------|
| Scraping strategy     | Uses public APIs (not fragile HTML parsing)                  |
| Modularity            | Separate files for scraping, storage, visualization           |
| Automation            | Cron-based scheduler for daily updates                        |
| Storage model         | HDF5 for time-series compatibility and deduplication         |
| Data structure        | Clean tabular format with typed fields                       |
| Validation            | Manual testing, fallback logic, and logging                  |

### ğŸ§  Academic & Industry Inspiration:

- **"Python Data Science Handbook"** by Jake VanderPlas â€“ for best practices in pandas, storage, and visualization.
- Open projects like `covid-data-scraper`, `cryptowatch`, and academic dashboards.
- IU's own course materials on scraping, data cleaning, and time-series data management.

### ğŸ“ Conclusion

Compared to other academic or open-source scraping projects, this approach is robust, reproducible, and extensible. It demonstrates not just technical capability, but thoughtful architecture and data quality awarenessâ€”core pillars of Data Wrangling and Data Quality.

## ğŸ“„ License

This project was developed for the academic module:
\
Data Quality and Data Wrangling

Python and SQL Programming and Data Analysis
\
IU Internationale Hochschule Akademie

The code is intended strictly for educational use and assessment.
\
Unauthorized reproduction or redistribution is not permitted.

## ğŸ“¬ Contact
Created by Diana Losch
ğŸ”— GitHub: @dlosch9225